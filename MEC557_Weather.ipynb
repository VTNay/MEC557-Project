{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VTNay/MEC557-Project/blob/Nay/MEC557_Weather.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f1ae401f",
      "metadata": {
        "id": "f1ae401f"
      },
      "source": [
        "# Projects\n",
        "\n",
        "[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/git/https%3A%2F%2Fgitlab.in2p3.fr%2Fenergy4climate%2Fpublic%2Feducation%2Fmachine_learning_for_climate_and_energy/master?filepath=book%2Fnotebooks%2Fprojects.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EfvEaXMCKKvc",
        "outputId": "1e863bac-2669-4538-b140-1779a5f92eb7"
      },
      "id": "EfvEaXMCKKvc",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "03d41164",
      "metadata": {
        "id": "03d41164"
      },
      "source": [
        "<div class=\"alert alert-block alert-warning\">\n",
        "    <b>Schedule</b>\n",
        "    \n",
        "- Ask your supervisors for the data if not already provided (it is not included in this repository).\n",
        "- Quick presentation.\n",
        "- Final project presentation.\n",
        "    \n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "01b8a8b3",
      "metadata": {
        "id": "01b8a8b3"
      },
      "source": [
        "<div class=\"alert alert-block alert-info\">\n",
        "    <b>One problematic, One dataset, One (or more) method(s)</b>\n",
        "    \n",
        "- Quality of the dataset is key.\n",
        "- Results on a clean notebook.\n",
        "- Explain which method(s) you used and why.\n",
        "- If a method fails, explain why.\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b309f16e",
      "metadata": {
        "id": "b309f16e"
      },
      "source": [
        "## Project: Weather station"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "912d896e",
      "metadata": {
        "id": "912d896e"
      },
      "source": [
        "<img alt=\"weather\" src=\"https://github.com/VTNay/MEC557-Project/blob/main/images/map.png?raw=1\" width=400>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2372e364",
      "metadata": {
        "id": "2372e364"
      },
      "source": [
        "- Suppose there are 5 weather stations that monitor the weather: Paris, Brest, London, Marseille and Berlin.\n",
        "- The weather station in Paris breaks down\n",
        "- Can we use the other stations to infer the weather in Paris"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "66ae5a0d",
      "metadata": {
        "id": "66ae5a0d"
      },
      "source": [
        "### Data set\n",
        "\n",
        "<img alt=\"weather\" src=\"https://github.com/VTNay/MEC557-Project/blob/main/images/annual_temperature.png?raw=1\" width=400>\n",
        "\n",
        "- Surface variables: skt, u10, v10, t2m, d2m, tcc, sp, tp, ssrd, blh\n",
        "- Temporal resolution: hourly\n",
        "- Spatial resolution: N/A"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "73ad9743",
      "metadata": {
        "id": "73ad9743"
      },
      "source": [
        "### First steps\n",
        "\n",
        "- Look at the correlations between variables.\n",
        "- What variable do I want to predict\n",
        "- What time scale am interested in?\n",
        "- Start with the easy predictions and move on to harder ones\n",
        "- Are there events that are more predictable than others?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "da094902",
      "metadata": {
        "id": "da094902",
        "outputId": "41f4b23c-6d08-4d28-b492-72d1f13e96e3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "40"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import xarray as xr\n",
        "from functools import reduce\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn import preprocessing\n",
        "\n",
        "paris_path = Path('/content/drive/My Drive/PHY557_Project/weather/paris')\n",
        "brest_path = Path('/content/drive/My Drive/PHY557_Project/weather/brest')\n",
        "london_path = Path('/content/drive/My Drive/PHY557_Project/weather/london')\n",
        "marseille_path = Path('/content/drive/My Drive/PHY557_Project/weather/marseille')\n",
        "berlin_path = Path('/content/drive/My Drive/PHY557_Project/weather/berlin')\n",
        "\n",
        "file_path = {'t2m': 't2m.nc', 'blh': 'blh.nc', 'd2m': 'd2m.nc', 'skt': 'skt.nc', 'sp': 'sp.nc', 'ssrd': 'ssrd.nc', 'tcc': 'tcc.nc', 'tp': 'tp.nc', 'u10': 'u10.nc', 'v10': 'v10.nc'}\n",
        "City_path = {'Paris': paris_path, 'Brest': brest_path, 'London': london_path, 'Marseille': marseille_path, 'Berlin': berlin_path}\n",
        "\n",
        "Weather_stations = {'Paris': [], 'Brest': [], 'London': [], 'Marseille': [], 'Berlin': []}\n",
        "for i in Weather_stations:\n",
        "  Weather_stations[i] = {'t2m': [], 'blh': [], 'd2m': [], 'skt': [], 'sp': [], 'ssrd': [], 'tcc': [], 'tp': [], 'u10': [], 'v10': []}\n",
        "\n",
        "for city in Weather_stations:\n",
        "  for i in Weather_stations[city]:\n",
        "    temp = xr.open_dataset(Path(City_path[city], file_path[i]))\n",
        "    temp = temp.to_dataframe()\n",
        "    if i == 'd2m' or i == 'blh':\n",
        "      temp = temp.droplevel([1,2])\n",
        "    else:\n",
        "      temp = temp.droplevel([0,1])\n",
        "    Weather_stations[city][i] = temp\n",
        "  #merge them into 1 dataframe\n",
        "  Weather_stations[city] = reduce(lambda left, right: pd.merge(left, right, left_index=True, right_index=True, how='outer'), Weather_stations[city].values())\n",
        "\n",
        "Berlin = Weather_stations['Berlin']\n",
        "Brest =  Weather_stations['Brest']\n",
        "London = Weather_stations['London']\n",
        "Paris = Weather_stations['Paris']\n",
        "Marseille = Weather_stations['Marseille']\n",
        "Paris = Paris[Paris.index < '2020-01-01 07:00:00'] #All dataframe has the same number of rows\n",
        "\n",
        "# Function to rename columns\n",
        "def rename_columns(df, prefix):\n",
        "    return df.rename(columns={col: f\"{prefix}_{col}\" for col in df.columns})\n",
        "# Rename columns of each DataFrame so that the feature in X will be 'Berlin_t2m', 'Berlin_u10', 'London_t2m',...\n",
        "Berlin = rename_columns(Berlin, 'Berlin')\n",
        "Brest = rename_columns(Brest, 'Brest')\n",
        "London = rename_columns(London, 'London')\n",
        "Marseille = rename_columns(Marseille, 'Marseille')\n",
        "\n",
        "#Data cleaning\n",
        "# Concatenate X = Berlin, Brest, London, Marseille and y = Paris\n",
        "combined = pd.concat([Berlin, Brest, London, Marseille, Paris], axis=1)\n",
        "# Drop NA values\n",
        "combined = combined.dropna()\n",
        "# Split them back into X and y\n",
        "X_raw = combined.iloc[:, :-10]  # X has 40 features\n",
        "y = combined.loc[:,'t2m']  # y is the temperature in Paris\n",
        "# Normalize X and y\n",
        "X_raw = (X_raw - X_raw.mean())/ X_raw.std()\n",
        "y = (y - y.mean())/y.std()\n",
        "# Number of years\n",
        "n_years = y.index.year.max() - y.index.year.min() + 1\n",
        "n_years"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y"
      ],
      "metadata": {
        "id": "Mwj8Ucf_9mkJ",
        "outputId": "248e725a-a615-46e1-e47f-d969bba179e7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "Mwj8Ucf_9mkJ",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "time\n",
              "1980-01-01 07:00:00    272.039154\n",
              "1980-01-01 08:00:00    272.022308\n",
              "1980-01-01 09:00:00    271.751892\n",
              "1980-01-01 10:00:00    274.506470\n",
              "1980-01-01 11:00:00    275.079346\n",
              "                          ...    \n",
              "2019-12-31 19:00:00    272.958130\n",
              "2019-12-31 20:00:00    272.240845\n",
              "2019-12-31 21:00:00    271.729919\n",
              "2019-12-31 22:00:00    273.190796\n",
              "2019-12-31 23:00:00    272.771423\n",
              "Freq: H, Name: Paris_t2m, Length: 350633, dtype: float32"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import xarray as xr\n",
        "from functools import reduce\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn import preprocessing\n",
        "\n",
        "# Define file paths for weather data of different cities\n",
        "weather_paths = {\n",
        "    'Paris': Path('/content/drive/My Drive/PHY557_Project/weather/paris'),\n",
        "    'Brest': Path('/content/drive/My Drive/PHY557_Project/weather/brest'),\n",
        "    'London': Path('/content/drive/My Drive/PHY557_Project/weather/london'),\n",
        "    'Marseille': Path('/content/drive/My Drive/PHY557_Project/weather/marseille'),\n",
        "    'Berlin': Path('/content/drive/My Drive/PHY557_Project/weather/berlin')\n",
        "}\n",
        "\n",
        "# Define the names of the files for each weather parameter\n",
        "file_names = {\n",
        "    't2m': 't2m.nc', 'blh': 'blh.nc', 'd2m': 'd2m.nc', 'skt': 'skt.nc',\n",
        "    'sp': 'sp.nc', 'ssrd': 'ssrd.nc', 'tcc': 'tcc.nc', 'tp': 'tp.nc',\n",
        "    'u10': 'u10.nc', 'v10': 'v10.nc'\n",
        "}\n",
        "\n",
        "# Initialize a dictionary to store weather data for each city\n",
        "weather_data = {city: {} for city in weather_paths}\n",
        "\n",
        "# Load and preprocess the data for each city\n",
        "for city, path in weather_paths.items():\n",
        "    for param, file_name in file_names.items():\n",
        "        # Load the dataset\n",
        "        dataset = xr.open_dataset(Path(path, file_name)).to_dataframe()\n",
        "        # Drop unnecessary levels based on the parameter\n",
        "        if param in ['d2m', 'blh']:\n",
        "            dataset = dataset.droplevel([1, 2])\n",
        "        else:\n",
        "            dataset = dataset.droplevel([0, 1])\n",
        "        # Store the processed data\n",
        "        weather_data[city][param] = dataset\n",
        "    # Merge data from different parameters into a single dataframe\n",
        "    weather_data[city] = reduce(lambda left, right: pd.merge(\n",
        "        left, right, left_index=True, right_index=True, how='outer'),\n",
        "        weather_data[city].values())\n",
        "\n",
        "# Function to rename columns with city prefix\n",
        "def rename_columns(df, prefix):\n",
        "    \"\"\"Rename columns with a city prefix.\"\"\"\n",
        "    return df.rename(columns={col: f\"{prefix}_{col}\" for col in df.columns})\n",
        "\n",
        "# Rename columns and concatenate data from all cities\n",
        "combined_data = pd.concat([rename_columns(weather_data[city], city) for city in weather_data], axis=1)\n",
        "\n",
        "# Data cleaning\n",
        "# Drop rows with missing values\n",
        "combined_data = combined_data.dropna()\n",
        "\n",
        "# Split the combined data into features (X) and target (y)\n",
        "X_raw = combined_data.iloc[:, :-10]  # Features from all cities except Paris\n",
        "y = combined_data['Paris_t2m']  # Target: temperature in Paris\n",
        "\n",
        "# Normalize features and target\n",
        "X_normalized = (X_raw - X_raw.mean()) / X_raw.std()\n",
        "y_normalized = (y - y.mean()) / y.std()\n",
        "\n",
        "# Calculate the number of years in the dataset\n",
        "n_years = y_normalized.index.year.max() - y_normalized.index.year.min() + 1"
      ],
      "metadata": {
        "id": "_AFDHEOp8-F1"
      },
      "id": "_AFDHEOp8-F1",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_normalized"
      ],
      "metadata": {
        "id": "9EJkB43-9eyZ",
        "outputId": "f3271f24-6db5-4812-91b6-aa7a1ffdcc1b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "9EJkB43-9eyZ",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "time\n",
              "1980-01-01 07:00:00   -1.730207\n",
              "1980-01-01 08:00:00   -1.732572\n",
              "1980-01-01 09:00:00   -1.770533\n",
              "1980-01-01 10:00:00   -1.383840\n",
              "1980-01-01 11:00:00   -1.303419\n",
              "                         ...   \n",
              "2019-12-31 19:00:00   -1.601199\n",
              "2019-12-31 20:00:00   -1.701893\n",
              "2019-12-31 21:00:00   -1.773618\n",
              "2019-12-31 22:00:00   -1.568537\n",
              "2019-12-31 23:00:00   -1.627409\n",
              "Freq: H, Name: Paris_t2m, Length: 350633, dtype: float32"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to rename columns\n",
        "def rename_columns(df, prefix):\n",
        "    return df.rename(columns={col: f\"{prefix}_{col}\" for col in df.columns})\n",
        "# Rename columns of each DataFrame so that the feature in X will be 'Berlin_t2m', 'Berlin_u10', 'London_t2m',...\n",
        "Berlin = rename_columns(Berlin, 'Berlin')\n",
        "Brest = rename_columns(Brest, 'Brest')\n",
        "London = rename_columns(London, 'London')\n",
        "Marseille = rename_columns(Marseille, 'Marseille')"
      ],
      "metadata": {
        "id": "KG4hH1ZCztTf"
      },
      "id": "KG4hH1ZCztTf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Data cleaning\n",
        "# Concatenate X = Berlin, Brest, London, Marseille and y = Paris\n",
        "combined = pd.concat([Berlin, Brest, London, Marseille, Paris], axis=1)\n",
        "# Drop NA values\n",
        "combined = combined.dropna()\n",
        "# Split them back into X and y\n",
        "X_raw = combined.iloc[:, :-10]  # X has 40 features\n",
        "y = combined.loc[:,'t2m']  # y is the temperature in Paris\n",
        "# Normalize X and y\n",
        "X_raw = (X_raw - X_raw.mean())/ X_raw.std()\n",
        "y = (y - y.mean())/y.std()\n",
        "# Number of years\n",
        "n_years = y.index.year.max() - y.index.year.min() + 1\n",
        "n_years"
      ],
      "metadata": {
        "id": "3mheGo1gm0T3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20719170-026e-4344-faca-cda4e78a8f8e"
      },
      "id": "3mheGo1gm0T3",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "40"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PCA**"
      ],
      "metadata": {
        "id": "E8xgsWcB_n48"
      },
      "id": "E8xgsWcB_n48"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lasso is a linear model that uses this cost function:\n",
        "\n",
        "$$\n",
        "\\frac{1}{2N_{\\text{training}}} \\sum_{i=1}^{N_{\\text{training}}} \\left( y^{(i)}_{\\text{real}} - y^{(i)}_{\\text{pred}} \\right)^2 + \\alpha \\sum_{j=1}^{n} |a_j|\n",
        "$$\n",
        "\n",
        "$a_j$ is the coefficient of the j-th feature. The final term is called $l_1$ penalty and $\\alpha$ is a hyperparameter that tunes the intensity of this penalty term. The higher the coefficient of a feature, the higher the value of the cost function. So, the idea of Lasso regression is to optimize the cost function reducing the absolute values of the coefficients.\n"
      ],
      "metadata": {
        "id": "1Fa2OtwzAsbH"
      },
      "id": "1Fa2OtwzAsbH"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LassoCV\n",
        "from sklearn.datasets import make_regression\n",
        "# Set number of splits for cross-validation - two years for each fold\n",
        "n_splits =  5 # We have 40 years in total\n",
        "\n",
        "# Initialize LassoCV, which will perform cross-validation\n",
        "lasso = LassoCV(cv=n_splits, random_state=0, max_iter=10000)\n",
        "\n",
        "# Fit the Lasso model to the data\n",
        "lasso.fit(X_raw, y)\n",
        "# Number of features remained\n",
        "n_features = 20\n",
        "# n_features features having the highest absolute value of coefficient can be considered as more important or keeped\n",
        "indices_top = np.argsort(np.abs(lasso.coef_))[-n_features:]\n",
        "selected_features = [X_raw.columns[i] for i in indices_top]\n",
        "\n",
        "# To reduce the feature set\n",
        "X = X_raw[selected_features]"
      ],
      "metadata": {
        "id": "BTs-B6vc_xfZ"
      },
      "id": "BTs-B6vc_xfZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Linear Regression**"
      ],
      "metadata": {
        "id": "AIvkLpQTPrpg"
      },
      "id": "AIvkLpQTPrpg"
    },
    {
      "cell_type": "code",
      "source": [
        "#Linear Regression for Paris_t2m with 40 features - the most naive approach\n",
        "# Import scikit-learn cross-validation function\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Call the Linear regressor\n",
        "lin = LinearRegression(fit_intercept= True)\n",
        "\n",
        "# Set number of splits for cross-validation - two years for each fold\n",
        "n_splits =  5 # We have 40 years in total\n",
        "\n",
        "# Initialize KFold\n",
        "kf = KFold(n_splits=n_splits)\n",
        "\n",
        "# Arrays to store scores\n",
        "train_scores = []\n",
        "test_scores = []\n",
        "\n",
        "for train_index, test_index in kf.split(X):\n",
        "    # Split data\n",
        "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
        "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
        "    # Fit model\n",
        "    lin.fit(X_train, y_train)\n",
        "\n",
        "    # Calculate R2 scores\n",
        "    train_score = lin.score(X_train,y_train)\n",
        "    test_score = lin.score(X_test, y_test)\n",
        "\n",
        "    # Append scores\n",
        "    train_scores.append(train_score)\n",
        "    test_scores.append(test_score)\n",
        "\n",
        "# Average R2 scores\n",
        "avg_train_score = np.mean(train_scores)\n",
        "avg_test_score = np.mean(test_scores)\n",
        "\n",
        "print(f\"Average R2 Score on Training Data: {avg_train_score}\")\n",
        "print(f\"Average R2 Score on Test Data: {avg_test_score}\")"
      ],
      "metadata": {
        "id": "eyZlJNIUn5kO",
        "outputId": "c26b83fd-edd8-4462-f19d-d69c5c880230",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "eyZlJNIUn5kO",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average R2 Score on Training Data: 0.9191834597452836\n",
            "Average R2 Score on Test Data: 0.9181068618513505\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_scores)\n",
        "print(test_scores)"
      ],
      "metadata": {
        "id": "XCSSQ_aP4hfl",
        "outputId": "fe98ace9-7999-48a9-b264-88a2463fe4ec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "XCSSQ_aP4hfl",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.9178100925659008, 0.9200567342719677, 0.9189771593618314, 0.9187203396985614, 0.9203529728281573]\n",
            "[0.9221752601135499, 0.9149210355507839, 0.9197156572378424, 0.9203415732840835, 0.9133807830704933]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2nd Degree Polynomial Regression**"
      ],
      "metadata": {
        "id": "7fnVe_oZ5Y9u"
      },
      "id": "7fnVe_oZ5Y9u"
    },
    {
      "cell_type": "code",
      "source": [
        "from inspect import modulesbyfile\n",
        "#Linear Regression for Paris_t2m with 40 features - the most naive approach\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "# Transform data to include polynomial features\n",
        "degree = 2\n",
        "polynomial_features = PolynomialFeatures(degree=degree, include_bias=True)\n",
        "linear_regression = LinearRegression()\n",
        "\n",
        "# Create a pipeline that includes both polynomial expansion and linear regression\n",
        "model = make_pipeline(polynomial_features, linear_regression)"
      ],
      "metadata": {
        "id": "2fVEpl6s5hDM"
      },
      "id": "2fVEpl6s5hDM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set number of splits for cross-validation - two years for each fold\n",
        "n_splits = 5 # We have 40 years in total\n",
        "\n",
        "# Initialize KFold\n",
        "kf = KFold(n_splits=n_splits)\n",
        "\n",
        "# Arrays to store scores\n",
        "train_scores = []\n",
        "test_scores = []\n",
        "\n",
        "for train_index, test_index in kf.split(X):\n",
        "    # Split data\n",
        "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
        "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
        "    # Fit model\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Calculate R2 scores\n",
        "    train_score = model.score(X_train,y_train)\n",
        "    test_score = model.score(X_test, y_test)\n",
        "\n",
        "    # Append scores\n",
        "    train_scores.append(train_score)\n",
        "    test_scores.append(test_score)\n",
        "\n",
        "# Average R2 scores\n",
        "avg_train_score = np.mean(train_scores)\n",
        "avg_test_score = np.mean(test_scores)\n",
        "\n",
        "print(f\"Average R2 Score on Training Data: {avg_train_score}\")\n",
        "print(f\"Average R2 Score on Test Data: {avg_test_score}\")"
      ],
      "metadata": {
        "id": "PRQwR-d4QW4S",
        "outputId": "4f230bee-509f-4ae6-cef4-3704965293ed",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "PRQwR-d4QW4S",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average R2 Score on Training Data: 0.9362718111277444\n",
            "Average R2 Score on Test Data: 0.9340951422262893\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3nd Degree Polynomial Regression**"
      ],
      "metadata": {
        "id": "HLVZpBNYXoxW"
      },
      "id": "HLVZpBNYXoxW"
    },
    {
      "cell_type": "code",
      "source": [
        "from inspect import modulesbyfile\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "#Linear Regression for Paris_t2m with 40 features - the most naive approach\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "# Transform data to include polynomial features\n",
        "degree = 3\n",
        "polynomial_features = PolynomialFeatures(degree=degree, include_bias=True)\n",
        "linear_regression = LinearRegression()\n",
        "\n",
        "# Create a pipeline that includes both polynomial expansion and linear regression\n",
        "model = make_pipeline(polynomial_features, linear_regression)"
      ],
      "metadata": {
        "id": "w9kDNr96XtmW"
      },
      "id": "w9kDNr96XtmW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3, random_state=0)\n",
        "model.fit(X_train, y_train)\n",
        "train_score = model.score(X_train,y_train)\n",
        "test_score = model.score(X_test, y_test)\n",
        "print(f\"Average R2 Score on Training Data: {train_score}\")\n",
        "print(f\"Average R2 Score on Test Data: {test_score}\")"
      ],
      "metadata": {
        "id": "8aIHiLu_gPft",
        "outputId": "babd9df3-dab3-4bbd-cb99-8f37cc4dbd76",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "8aIHiLu_gPft",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average R2 Score on Training Data: 0.9451980423935095\n",
            "Average R2 Score on Test Data: 0.9444146196582686\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set number of splits for cross-validation - two years for each fold\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "n_splits = 5 # We have n_features years in total\n",
        "\n",
        "# Initialize KFold\n",
        "kf = KFold(n_splits=n_splits)\n",
        "\n",
        "# Arrays to store scores\n",
        "train_scores = []\n",
        "test_scores = []\n",
        "\n",
        "for train_index, test_index in kf.split(X):\n",
        "    # Split data\n",
        "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
        "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
        "    # Fit model\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Calculate R2 scores\n",
        "    train_score = model.score(X_train,y_train)\n",
        "    test_score = model.score(X_test, y_test)\n",
        "\n",
        "    # Append scores\n",
        "    train_scores.append(train_score)\n",
        "    test_scores.append(test_score)\n",
        "\n",
        "# Average R2 scores\n",
        "avg_train_score = np.mean(train_scores)\n",
        "avg_test_score = np.mean(test_scores)\n",
        "\n",
        "print(f\"Average R2 Score on Training Data: {avg_train_score}\")\n",
        "print(f\"Average R2 Score on Test Data: {avg_test_score}\")"
      ],
      "metadata": {
        "id": "EUglJMuGXvaM",
        "outputId": "921236d2-7497-4efa-b638-ce40974258fb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "EUglJMuGXvaM",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average R2 Score on Training Data: 0.9456672090526222\n",
            "Average R2 Score on Test Data: 0.9385114223385068\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fucntion ##"
      ],
      "metadata": {
        "id": "Bg_10e6ephP5"
      },
      "id": "Bg_10e6ephP5"
    },
    {
      "cell_type": "code",
      "source": [
        "from inspect import modulesbyfile\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "#Linear Regression for Paris_t2m with 40 features - the most naive approach\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.pipeline import make_pipeline\n",
        "#Ridge\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "def poly_ridge(degree, X_cv, y_cv, alpha):\n",
        "  # # Transform data to include polynomial features\n",
        "  # polynomial_features = PolynomialFeatures(degree=degree, include_bias=True)\n",
        "\n",
        "  # # Create a pipeline that includes both polynomial expansion and linear regression\n",
        "  # model = make_pipeline(polynomial_features, linear_regression)\n",
        "\n",
        "\n",
        "  #model.fit(X_train, y_train)\n",
        "  #train_score = model.score(X_train,y_train)\n",
        "  #test_score = model.score(X_test, y_test)\n",
        "\n",
        "  #print(f\"Average R2 Score on Training Data: {train_score}\")\n",
        "  #print(f\"Average R2 Score on Test Data: {test_score}\")\n",
        "\n",
        "  # n_splits = 5 # We have 0.75 x n_years = 30 years in total\n",
        "\n",
        "  # # Initialize KFold\n",
        "  # kf = KFold(n_splits=n_splits)\n",
        "\n",
        "  # # Arrays to store scores\n",
        "  # train_scores = []\n",
        "  # test_scores = []\n",
        "  '''\n",
        "  for train_index, test_index in kf.split(X_cv):\n",
        "      # Split data\n",
        "      X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
        "      y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
        "      # Fit model\n",
        "      model.fit(X_train, y_train)\n",
        "\n",
        "      # Calculate R2 scores\n",
        "      train_score = model.score(X_train,y_train)\n",
        "      test_score = model.score(X_test, y_test)\n",
        "\n",
        "      # Append scores\n",
        "      train_scores.append(train_score)\n",
        "      test_scores.append(test_score)\n",
        "\n",
        "  # Average R2 scores\n",
        "  avg_train_score = np.mean(train_scores)\n",
        "  avg_test_score = np.mean(test_scores)\n",
        "\n",
        "  print(f\"Average R2 Score on Training Data: {avg_train_score}\")\n",
        "  print(f\"Average R2 Score on Test Data: {avg_test_score}\")\n",
        "  '''\n",
        "  #Ridge regression\n",
        "  # Call the Ridge regressor\n",
        "  #reg_class = Ridge\n",
        "\n",
        "  # Number of test years\n",
        "  #N_TEST_YEARS = 8\n",
        "  # Number of test days = number of test columns\n",
        "  #n_test = 365 * N_TEST_YEARS # 365 days per year\n",
        "\n",
        "  # Define array of regularization-parameter values\n",
        "  #alpha = np.linspace(0, 80, 80)\n",
        "\n",
        "  # Select cross validation data\n",
        "  # X_cv = X[:-n_test]\n",
        "  # y_cv = y[:-n_test]\n",
        "\n",
        "  # # Select test set for later\n",
        "  # X_test = X[-n_test:]\n",
        "  # y_test = y[-n_test:]\n",
        "\n",
        "  # Set number of splits for cross-validation - two years for each fold\n",
        "  #n_splits_cv = (n_years - N_TEST_YEARS)//2\n",
        "  n_splits_cv = 5\n",
        "\n",
        "  # Declare empty arrays in which to store r2 scores and coefficients\n",
        "  r2_validation = np.empty(alpha.shape)\n",
        "  coefs = np.empty((len(alpha), X.shape[1]))\n",
        "  #r2_test = np.empty(alpha.shape)\n",
        "\n",
        "\n",
        "  # Loop over regularization-parameter values\n",
        "  for k, complexity in enumerate(alpha):\n",
        "      # Transform data to include polynomial features\n",
        "      polynomial_features = PolynomialFeatures(degree=degree, include_bias=True)\n",
        "      # Define the Ridge estimator for particular regularization-parameter value\n",
        "      reg = Ridge(alpha=complexity)\n",
        "      # Create a pipeline that includes both polynomial expansion and ridge regression\n",
        "      model = make_pipeline(polynomial_features, reg)\n",
        "      # Get r2 test scores from k-fold cross-validation\n",
        "      r2_validation_arr = cross_val_score(model, X_cv, y_cv, cv=n_splits_cv)\n",
        "\n",
        "      # Get r2 expected prediction score by averaging over test scores\n",
        "      r2_validation[k] = r2_validation_arr.mean()\n",
        "\n",
        "      # Save coefficients\n",
        "      # reg.fit(X_cv, y_cv)\n",
        "      # coefs[k] = reg.coef_\n",
        "\n",
        "      # Get r2 test error\n",
        "      #r2_test[k] = reg.score(X_test, y_test)\n",
        "\n",
        "\n",
        "    # Get the best values of the regularization parameter, prediction R2 and coefficients\n",
        "  i_best = np.argmax(r2_validation)\n",
        "  alpha_best = alpha[i_best]\n",
        "  r2_validation_best = r2_validation[i_best]\n",
        "  return r2_validation, i_best\n"
      ],
      "metadata": {
        "id": "3VNrUmKrYzhE"
      },
      "id": "3VNrUmKrYzhE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Degree 1\n",
        "degree = 1\n",
        "X_cv, X_test, y_cv, y_test = train_test_split(X, y, test_size=.25, random_state=0)\n",
        "alpha = np.linspace(0, 80, 80)\n",
        "r2_validation, i_best = poly_ridge(degree, X_cv, y_cv, alpha)\n",
        "best_model = Ridge(alpha = alpha[i_best])\n",
        "best_model.fit(X_cv, y_cv)\n",
        "R2 = best_model.score(X_test, y_test)\n",
        "print(r2_validation)\n",
        "print(R2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fM1GO0iWvLN-",
        "outputId": "a624810e-493f-432a-ee2d-87bc46ad1f25"
      },
      "id": "fM1GO0iWvLN-",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.91897914 0.9189792  0.9189792  0.91897919 0.91897919 0.91897919\n",
            " 0.91897919 0.91897918 0.91897918 0.91897917 0.91897917 0.91897916\n",
            " 0.91897915 0.91897914 0.91897913 0.91897912 0.91897911 0.9189791\n",
            " 0.91897909 0.91897908 0.91897907 0.91897905 0.91897904 0.91897902\n",
            " 0.918979   0.91897899 0.91897897 0.91897895 0.91897893 0.91897891\n",
            " 0.91897889 0.91897887 0.91897885 0.91897883 0.9189788  0.91897878\n",
            " 0.91897876 0.91897873 0.91897871 0.91897868 0.91897865 0.91897863\n",
            " 0.91897859 0.91897857 0.91897854 0.9189785  0.91897848 0.91897844\n",
            " 0.91897841 0.91897838 0.91897834 0.91897831 0.91897827 0.91897823\n",
            " 0.9189782  0.91897816 0.91897813 0.91897809 0.91897805 0.91897801\n",
            " 0.91897797 0.91897793 0.91897789 0.91897784 0.9189778  0.91897775\n",
            " 0.91897771 0.91897767 0.91897762 0.91897757 0.91897752 0.91897748\n",
            " 0.91897743 0.91897738 0.91897733 0.91897728 0.91897723 0.91897718\n",
            " 0.91897713 0.91897707]\n",
            "0.9196585732899049\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Degree 2\n",
        "degree = 2\n",
        "X_cv, X_test, y_cv, y_test = train_test_split(X, y, test_size=.25, random_state=0)\n",
        "alpha = np.linspace(0, 80, 80)\n",
        "r2_validation, i_best = poly_ridge(degree, X_cv, y_cv, alpha)\n",
        "best_model = Ridge(alpha = alpha[i_best])\n",
        "best_model.fit(X_cv, y_cv)\n",
        "R2 = best_model.score(X_test, y_test)\n",
        "print(r2_validation)\n",
        "print(R2)"
      ],
      "metadata": {
        "id": "dMSyPMaE05y-",
        "outputId": "a37f3a2a-217a-4582-9845-018847c8fcee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "dMSyPMaE05y-",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.93595851 0.93595815 0.93595816 0.93595812 0.93595805 0.93595794\n",
            " 0.93595783 0.93595768 0.93595754 0.93595735 0.93595716 0.93595694\n",
            " 0.93595668 0.93595646 0.93595621 0.93595592 0.93595565 0.93595536\n",
            " 0.93595505 0.93595474 0.93595443 0.93595413 0.93595381 0.93595347\n",
            " 0.93595313 0.9359528  0.93595244 0.93595209 0.93595172 0.93595136\n",
            " 0.93595103 0.93595067 0.93595029 0.93594991 0.93594954 0.93594918\n",
            " 0.93594879 0.93594843 0.93594805 0.93594766 0.93594728 0.93594691\n",
            " 0.93594651 0.93594614 0.93594575 0.93594536 0.93594496 0.9359446\n",
            " 0.93594419 0.93594379 0.93594343 0.93594301 0.93594262 0.93594224\n",
            " 0.93594186 0.93594144 0.93594103 0.93594066 0.93594025 0.93593986\n",
            " 0.93593946 0.93593904 0.93593867 0.93593828 0.93593785 0.93593748\n",
            " 0.93593704 0.93593666 0.93593624 0.93593585 0.93593544 0.93593506\n",
            " 0.93593462 0.93593425 0.93593383 0.93593343 0.93593302 0.93593263\n",
            " 0.93593222 0.93593181]\n",
            "0.9196585873142077\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Degree 3\n",
        "degree = 3\n",
        "X_cv, X_test, y_cv, y_test = train_test_split(X, y, test_size=.25, random_state=0)\n",
        "alpha = np.linspace(0, 80, 80)\n",
        "r2_validation, i_best = poly_ridge(degree, X_cv, y_cv, alpha)\n",
        "best_model = Ridge(alpha = alpha[i_best])\n",
        "best_model.fit(X_cv, y_cv)\n",
        "R2 = best_model.score(X_test, y_test)\n",
        "print(r2_validation)\n",
        "print(R2)"
      ],
      "metadata": {
        "id": "nFZ7m5Qa1Evo"
      },
      "id": "nFZ7m5Qa1Evo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ridge Regression**"
      ],
      "metadata": {
        "id": "TZ0DRq1tPjWD"
      },
      "id": "TZ0DRq1tPjWD"
    },
    {
      "cell_type": "code",
      "source": [
        "#Ridge regression\n",
        "# Import Ridge\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.model_selection import cross_val_score\n",
        "# Call the Ridge regressor\n",
        "reg_class = Ridge\n",
        "\n",
        "# Number of test years\n",
        "N_TEST_YEARS = 8\n",
        "# Number of test days = number of test columns\n",
        "n_test = 365 * N_TEST_YEARS # 365 days per year\n",
        "\n",
        "# Define array of regularization-parameter values\n",
        "alpha = np.linspace(0, 80, 80)\n",
        "\n",
        "# Select cross validation data\n",
        "X_cv = X[:-n_test]\n",
        "y_cv = y[:-n_test]\n",
        "\n",
        "# Select test set for later\n",
        "X_test = X[-n_test:]\n",
        "y_test = y[-n_test:]\n",
        "\n",
        "# Set number of splits for cross-validation - two years for each fold\n",
        "n_splits_cv = (n_years - N_TEST_YEARS)//2\n",
        "\n",
        "# Declare empty arrays in which to store r2 scores and coefficients\n",
        "r2_validation = np.empty(alpha.shape)\n",
        "coefs = np.empty((len(alpha), X.shape[1]))\n",
        "r2_test = np.empty(alpha.shape)\n",
        "\n",
        "\n",
        "# Loop over regularization-parameter values\n",
        "for k, complexity in enumerate(alpha):\n",
        "    # Define the Ridge estimator for particular regularization-parameter value\n",
        "    reg = reg_class(alpha=complexity)\n",
        "\n",
        "    # Get r2 test scores from k-fold cross-validation\n",
        "    r2_validation_arr = cross_val_score(reg, X_cv, y_cv, cv=n_splits_cv)\n",
        "\n",
        "    # Get r2 expected prediction score by averaging over test scores\n",
        "    r2_validation[k] = r2_validation_arr.mean()\n",
        "\n",
        "    # Save coefficients\n",
        "    reg.fit(X_cv, y_cv)\n",
        "    coefs[k] = reg.coef_\n",
        "\n",
        "    # Get r2 test error\n",
        "    r2_test[k] = reg.score(X_test, y_test)\n",
        "\n",
        "\n",
        "# Get the best values of the regularization parameter, prediction R2 and coefficients\n",
        "i_best = np.argmax(r2_validation)\n",
        "alpha_best = alpha[i_best]\n",
        "r2_validation_best = r2_validation[i_best]\n",
        "coefs_best = coefs[i_best]\n",
        "r2_test_best = r2_test[i_best]"
      ],
      "metadata": {
        "id": "c0pmD7-JYeii"
      },
      "id": "c0pmD7-JYeii",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(r2_validation)"
      ],
      "metadata": {
        "id": "acleenFpSoLK"
      },
      "id": "acleenFpSoLK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot validation curve\n",
        "complexity_label = r'$\\alpha$'\n",
        "plt.figure()\n",
        "plt.plot(alpha, r2_validation, label = 'Train R2')\n",
        "plt.legend()\n",
        "plt.xlabel(complexity_label)\n",
        "plt.ylabel(r'$R^2$')\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(alpha, r2_test, label = 'Test R2')\n",
        "plt.xlabel(complexity_label)\n",
        "plt.ylabel(r'$R^2$')\n",
        "plt.legend()\n",
        "_ = plt.title(r'Best $R^2 train$: {:.3} for $\\alpha$ = {:.1e} and $R^2 test$ : {:.3}'.format(\n",
        "    r2_validation_best, alpha_best, r2_test_best))\n",
        "_ = plt.xlim(alpha[[0, -1]])\n"
      ],
      "metadata": {
        "id": "DeN1ZSKLrT7_"
      },
      "id": "DeN1ZSKLrT7_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the Ridge estimator for best regularization parameter value\n",
        "reg = reg_class(alpha=alpha_best)\n",
        "\n",
        "# Fit on train data\n",
        "reg.fit(X_cv, y_cv)\n",
        "\n",
        "# Test on test data\n",
        "r2_test = reg.score(X_test, y_test)\n",
        "\n",
        "print('Test R2: {:.3f}'.format(r2_test))"
      ],
      "metadata": {
        "id": "m00xD4fuRw3l"
      },
      "id": "m00xD4fuRw3l",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "ad79471d",
      "metadata": {
        "id": "ad79471d"
      },
      "source": [
        "***\n",
        "## Credit\n",
        "\n",
        "[//]: # \"This notebook is part of [E4C Interdisciplinary Center - Education](https://gitlab.in2p3.fr/energy4climate/public/education).\"\n",
        "Contributors include Bruno Deremble and Alexis Tantet.\n",
        "Several slides and images are taken from the very good [Scikit-learn course](https://inria.github.io/scikit-learn-mooc/).\n",
        "\n",
        "<br>\n",
        "\n",
        "<div style=\"display: flex; height: 70px\">\n",
        "    \n",
        "<img alt=\"Logo LMD\" src=\"https://github.com/VTNay/MEC557-Project/blob/main/images/logos/logo_lmd.jpg?raw=1\" style=\"display: inline-block\"/>\n",
        "\n",
        "<img alt=\"Logo IPSL\" src=\"https://github.com/VTNay/MEC557-Project/blob/main/images/logos/logo_ipsl.png?raw=1\" style=\"display: inline-block\"/>\n",
        "\n",
        "<img alt=\"Logo E4C\" src=\"https://github.com/VTNay/MEC557-Project/blob/main/images/logos/logo_e4c_final.png?raw=1\" style=\"display: inline-block\"/>\n",
        "\n",
        "<img alt=\"Logo EP\" src=\"https://github.com/VTNay/MEC557-Project/blob/main/images/logos/logo_ep.png?raw=1\" style=\"display: inline-block\"/>\n",
        "\n",
        "<img alt=\"Logo SU\" src=\"https://github.com/VTNay/MEC557-Project/blob/main/images/logos/logo_su.png?raw=1\" style=\"display: inline-block\"/>\n",
        "\n",
        "<img alt=\"Logo ENS\" src=\"https://github.com/VTNay/MEC557-Project/blob/main/images/logos/logo_ens.jpg?raw=1\" style=\"display: inline-block\"/>\n",
        "\n",
        "<img alt=\"Logo CNRS\" src=\"https://github.com/VTNay/MEC557-Project/blob/main/images/logos/logo_cnrs.png?raw=1\" style=\"display: inline-block\"/>\n",
        "    \n",
        "</div>\n",
        "\n",
        "<hr>\n",
        "\n",
        "<div style=\"display: flex\">\n",
        "    <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-sa/4.0/\"><img alt=\"Creative Commons License\" style=\"border-width:0; margin-right: 10px\" src=\"https://i.creativecommons.org/l/by-sa/4.0/88x31.png\" /></a>\n",
        "    <br>This work is licensed under a &nbsp; <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-sa/4.0/\">Creative Commons Attribution-ShareAlike 4.0 International License</a>.\n",
        "</div>"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "latex_envs": {
      "LaTeX_envs_menu_present": true,
      "autoclose": true,
      "autocomplete": false,
      "bibliofile": "biblio.bib",
      "cite_by": "apalike",
      "current_citInitial": 1,
      "eqLabelWithNumbers": true,
      "eqNumInitial": 1,
      "hotkeys": {
        "equation": "Ctrl-E",
        "itemize": "Ctrl-I"
      },
      "labels_anchors": false,
      "latex_user_defs": false,
      "report_style_numbering": false,
      "user_envs_cfg": false
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}