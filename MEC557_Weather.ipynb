{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VTNay/MEC557-Project/blob/Nay/MEC557_Weather.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f1ae401f",
      "metadata": {
        "id": "f1ae401f"
      },
      "source": [
        "# Projects\n",
        "\n",
        "[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/git/https%3A%2F%2Fgitlab.in2p3.fr%2Fenergy4climate%2Fpublic%2Feducation%2Fmachine_learning_for_climate_and_energy/master?filepath=book%2Fnotebooks%2Fprojects.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EfvEaXMCKKvc",
        "outputId": "2d7cf253-f1c1-43ab-c7a2-90bbc4f9be26"
      },
      "id": "EfvEaXMCKKvc",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "03d41164",
      "metadata": {
        "id": "03d41164"
      },
      "source": [
        "<div class=\"alert alert-block alert-warning\">\n",
        "    <b>Schedule</b>\n",
        "    \n",
        "- Ask your supervisors for the data if not already provided (it is not included in this repository).\n",
        "- Quick presentation.\n",
        "- Final project presentation.\n",
        "    \n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "01b8a8b3",
      "metadata": {
        "id": "01b8a8b3"
      },
      "source": [
        "<div class=\"alert alert-block alert-info\">\n",
        "    <b>One problematic, One dataset, One (or more) method(s)</b>\n",
        "    \n",
        "- Quality of the dataset is key.\n",
        "- Results on a clean notebook.\n",
        "- Explain which method(s) you used and why.\n",
        "- If a method fails, explain why.\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b309f16e",
      "metadata": {
        "id": "b309f16e"
      },
      "source": [
        "## Project: Weather station"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "912d896e",
      "metadata": {
        "id": "912d896e"
      },
      "source": [
        "<img alt=\"weather\" src=\"https://github.com/VTNay/MEC557-Project/blob/main/images/map.png?raw=1\" width=400>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2372e364",
      "metadata": {
        "id": "2372e364"
      },
      "source": [
        "- Suppose there are 5 weather stations that monitor the weather: Paris, Brest, London, Marseille and Berlin.\n",
        "- The weather station in Paris breaks down\n",
        "- Can we use the other stations to infer the weather in Paris"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "66ae5a0d",
      "metadata": {
        "id": "66ae5a0d"
      },
      "source": [
        "### Data set\n",
        "\n",
        "<img alt=\"weather\" src=\"https://github.com/VTNay/MEC557-Project/blob/main/images/annual_temperature.png?raw=1\" width=400>\n",
        "\n",
        "- Surface variables: skt, u10, v10, t2m, d2m, tcc, sp, tp, ssrd, blh\n",
        "- Temporal resolution: hourly\n",
        "- Spatial resolution: N/A"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "73ad9743",
      "metadata": {
        "id": "73ad9743"
      },
      "source": [
        "### First steps\n",
        "\n",
        "- Look at the correlations between variables.\n",
        "- What variable do I want to predict\n",
        "- What time scale am interested in?\n",
        "- Start with the easy predictions and move on to harder ones\n",
        "- Are there events that are more predictable than others?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "da094902",
      "metadata": {
        "id": "da094902"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import xarray as xr\n",
        "from functools import reduce\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn import preprocessing\n",
        "\n",
        "paris_path = Path('/content/drive/My Drive/PHY557_Project/weather/paris')\n",
        "brest_path = Path('/content/drive/My Drive/PHY557_Project/weather/brest')\n",
        "london_path = Path('/content/drive/My Drive/PHY557_Project/weather/london')\n",
        "marseille_path = Path('/content/drive/My Drive/PHY557_Project/weather/marseille')\n",
        "berlin_path = Path('/content/drive/My Drive/PHY557_Project/weather/berlin')\n",
        "\n",
        "file_path = {'t2m': 't2m.nc', 'blh': 'blh.nc', 'd2m': 'd2m.nc', 'skt': 'skt.nc', 'sp': 'sp.nc', 'ssrd': 'ssrd.nc', 'tcc': 'tcc.nc', 'tp': 'tp.nc', 'u10': 'u10.nc', 'v10': 'v10.nc'}\n",
        "City_path = {'Paris': paris_path, 'Brest': brest_path, 'London': london_path, 'Marseille': marseille_path, 'Berlin': berlin_path}\n",
        "\n",
        "Weather_stations = {'Paris': [], 'Brest': [], 'London': [], 'Marseille': [], 'Berlin': []}\n",
        "for i in Weather_stations:\n",
        "  Weather_stations[i] = {'t2m': [], 'blh': [], 'd2m': [], 'skt': [], 'sp': [], 'ssrd': [], 'tcc': [], 'tp': [], 'u10': [], 'v10': []}\n",
        "\n",
        "for city in Weather_stations:\n",
        "  for i in Weather_stations[city]:\n",
        "    temp = xr.open_dataset(Path(City_path[city], file_path[i]))\n",
        "    temp = temp.to_dataframe()\n",
        "    if i == 'd2m' or i == 'blh':\n",
        "      temp = temp.droplevel([1,2])\n",
        "    else:\n",
        "      temp = temp.droplevel([0,1])\n",
        "    Weather_stations[city][i] = temp\n",
        "  #merge them into 1 dataframe\n",
        "  Weather_stations[city] = reduce(lambda left, right: pd.merge(left, right, left_index=True, right_index=True, how='outer'), Weather_stations[city].values())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Berlin = Weather_stations['Berlin']\n",
        "Brest =  Weather_stations['Brest']\n",
        "London = Weather_stations['London']\n",
        "Paris = Weather_stations['Paris']\n",
        "Marseille = Weather_stations['Marseille']\n",
        "Paris = Paris[Paris.index < '2020-01-01 07:00:00'] #All dataframe has the same number of rows"
      ],
      "metadata": {
        "id": "u-fkGxxJw0WX"
      },
      "id": "u-fkGxxJw0WX",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to rename columns\n",
        "def rename_columns(df, prefix):\n",
        "    return df.rename(columns={col: f\"{prefix}_{col}\" for col in df.columns})\n",
        "# Rename columns of each DataFrame so that the feature in X will be 'Berlin_t2m', 'Berlin_u10', 'London_t2m',...\n",
        "Berlin = rename_columns(Berlin, 'Berlin')\n",
        "Brest = rename_columns(Brest, 'Brest')\n",
        "London = rename_columns(London, 'London')\n",
        "Marseille = rename_columns(Marseille, 'Marseille')"
      ],
      "metadata": {
        "id": "KG4hH1ZCztTf"
      },
      "id": "KG4hH1ZCztTf",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Data cleaning\n",
        "# Concatenate X = Berlin, Brest, London, Marseille and y = Paris\n",
        "combined = pd.concat([Berlin, Brest, London, Marseille, Paris], axis=1)\n",
        "# Drop NA values\n",
        "combined = combined.dropna()\n",
        "# Split them back into X and y\n",
        "X = combined.iloc[:, :-10]  # X has 40 features\n",
        "y = combined.loc[:,'t2m']  # y is the temperature in Paris\n",
        "# Normalize X and y\n",
        "X = (X - X.mean())/ X.std()\n",
        "y = (y - y.mean())/y.std()\n",
        "# Number of years\n",
        "n_years = y.index.year.max() - y.index.year.min() + 1\n",
        "n_years"
      ],
      "metadata": {
        "id": "3mheGo1gm0T3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f1ddf90-7ed8-4b88-cd83-020c06ad0826"
      },
      "id": "3mheGo1gm0T3",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "40"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PCA**"
      ],
      "metadata": {
        "id": "E8xgsWcB_n48"
      },
      "id": "E8xgsWcB_n48"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lasso is a linear model that uses this cost function:\n",
        "\n",
        "$$\n",
        "\\frac{1}{2N_{\\text{training}}} \\sum_{i=1}^{N_{\\text{training}}} \\left( y^{(i)}_{\\text{real}} - y^{(i)}_{\\text{pred}} \\right)^2 + \\alpha \\sum_{j=1}^{n} |a_j|\n",
        "$$\n",
        "\n",
        "$a_j$ is the coefficient of the j-th feature. The final term is called $l_1$ penalty and $\\alpha$ is a hyperparameter that tunes the intensity of this penalty term. The higher the coefficient of a feature, the higher the value of the cost function. So, the idea of Lasso regression is to optimize the cost function reducing the absolute values of the coefficients.\n"
      ],
      "metadata": {
        "id": "1Fa2OtwzAsbH"
      },
      "id": "1Fa2OtwzAsbH"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LassoCV\n",
        "from sklearn.datasets import make_regression\n",
        "# Set number of splits for cross-validation - two years for each fold\n",
        "n_splits =  5 # We have 40 years in total\n",
        "\n",
        "# Initialize LassoCV, which will perform cross-validation\n",
        "lasso = LassoCV(cv=n_splits, random_state=0, max_iter=10000)\n",
        "\n",
        "# Fit the Lasso model to the data\n",
        "lasso.fit(X, y)\n",
        "\n",
        "# Features with a coefficient of 0 can be considered as less important or removed\n",
        "coefficients = lasso.coef_\n",
        "\n",
        "# Get the mask of selected features (non-zero coefficients)\n",
        "selected_features = coefficients != 0\n",
        "\n",
        "# To reduce the feature set\n",
        "X_reduced = X[:, selected_features]"
      ],
      "metadata": {
        "id": "BTs-B6vc_xfZ",
        "outputId": "176a05df-e568-4889-cc6d-fc81bfdbda64",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 582
        }
      },
      "id": "BTs-B6vc_xfZ",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "InvalidIndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3801\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3802\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3803\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: '(slice(None, None, None), array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True, False,  True, False,  True,\n        True,  True,  True,  True,  True, False,  True,  True,  True,\n       False,  True,  True,  True]))' is an invalid key",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mInvalidIndexError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-f8168faf71f5>\u001b[0m in \u001b[0;36m<cell line: 19>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# To reduce the feature set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mX_reduced\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mselected_features\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3805\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3806\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3807\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3808\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3809\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3807\u001b[0m                 \u001b[0;31m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3808\u001b[0m                 \u001b[0;31m#  the TypeError.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3809\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_indexing_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3810\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3811\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m_check_indexing_error\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   5923\u001b[0m             \u001b[0;31m# if key is not a scalar, directly raise an error (the code below\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5924\u001b[0m             \u001b[0;31m# would convert to numpy arrays and raise later any way) - GH29926\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5925\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mInvalidIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5926\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5927\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mcache_readonly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidIndexError\u001b[0m: (slice(None, None, None), array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True, False,  True, False,  True,\n        True,  True,  True,  True,  True, False,  True,  True,  True,\n       False,  True,  True,  True]))"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Linear Regression**"
      ],
      "metadata": {
        "id": "AIvkLpQTPrpg"
      },
      "id": "AIvkLpQTPrpg"
    },
    {
      "cell_type": "code",
      "source": [
        "#Linear Regression for Paris_t2m with 40 features - the most naive approach\n",
        "# Import scikit-learn cross-validation function\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Call the Linear regressor\n",
        "lin = LinearRegression(fit_intercept= True)\n",
        "\n",
        "# Set number of splits for cross-validation - two years for each fold\n",
        "n_splits =  5 # We have 40 years in total\n",
        "\n",
        "# Initialize KFold\n",
        "kf = KFold(n_splits=n_splits)\n",
        "\n",
        "# Arrays to store scores\n",
        "train_scores = []\n",
        "test_scores = []\n",
        "\n",
        "for train_index, test_index in kf.split(X):\n",
        "    # Split data\n",
        "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
        "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
        "    # Fit model\n",
        "    lin.fit(X_train, y_train)\n",
        "\n",
        "    # Calculate R2 scores\n",
        "    train_score = lin.score(X_train,y_train)\n",
        "    test_score = lin.score(X_test, y_test)\n",
        "\n",
        "    # Append scores\n",
        "    train_scores.append(train_score)\n",
        "    test_scores.append(test_score)\n",
        "\n",
        "# Average R2 scores\n",
        "avg_train_score = np.mean(train_scores)\n",
        "avg_test_score = np.mean(test_scores)\n",
        "\n",
        "print(f\"Average R2 Score on Training Data: {avg_train_score}\")\n",
        "print(f\"Average R2 Score on Test Data: {avg_test_score}\")"
      ],
      "metadata": {
        "id": "eyZlJNIUn5kO"
      },
      "id": "eyZlJNIUn5kO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_scores)\n",
        "print(test_scores)"
      ],
      "metadata": {
        "id": "XCSSQ_aP4hfl"
      },
      "id": "XCSSQ_aP4hfl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2nd Degree Polynomial Regression**"
      ],
      "metadata": {
        "id": "7fnVe_oZ5Y9u"
      },
      "id": "7fnVe_oZ5Y9u"
    },
    {
      "cell_type": "code",
      "source": [
        "from inspect import modulesbyfile\n",
        "#Linear Regression for Paris_t2m with 40 features - the most naive approach\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "# Transform data to include polynomial features\n",
        "degree = 2\n",
        "polynomial_features = PolynomialFeatures(degree=degree, include_bias=True)\n",
        "linear_regression = LinearRegression()\n",
        "\n",
        "# Create a pipeline that includes both polynomial expansion and linear regression\n",
        "model = make_pipeline(polynomial_features, linear_regression)"
      ],
      "metadata": {
        "id": "2fVEpl6s5hDM"
      },
      "id": "2fVEpl6s5hDM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set number of splits for cross-validation - two years for each fold\n",
        "n_splits = 5 # We have 40 years in total\n",
        "\n",
        "# Initialize KFold\n",
        "kf = KFold(n_splits=n_splits)\n",
        "\n",
        "# Arrays to store scores\n",
        "train_scores = []\n",
        "test_scores = []\n",
        "\n",
        "for train_index, test_index in kf.split(X):\n",
        "    # Split data\n",
        "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
        "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
        "    # Fit model\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Calculate R2 scores\n",
        "    train_score = model.score(X_train,y_train)\n",
        "    test_score = model.score(X_test, y_test)\n",
        "\n",
        "    # Append scores\n",
        "    train_scores.append(train_score)\n",
        "    test_scores.append(test_score)\n",
        "\n",
        "# Average R2 scores\n",
        "avg_train_score = np.mean(train_scores)\n",
        "avg_test_score = np.mean(test_scores)\n",
        "\n",
        "print(f\"Average R2 Score on Training Data: {avg_train_score}\")\n",
        "print(f\"Average R2 Score on Test Data: {avg_test_score}\")"
      ],
      "metadata": {
        "id": "PRQwR-d4QW4S"
      },
      "id": "PRQwR-d4QW4S",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3nd Degree Polynomial Regression**"
      ],
      "metadata": {
        "id": "HLVZpBNYXoxW"
      },
      "id": "HLVZpBNYXoxW"
    },
    {
      "cell_type": "code",
      "source": [
        "from inspect import modulesbyfile\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "#Linear Regression for Paris_t2m with 40 features - the most naive approach\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "# Transform data to include polynomial features\n",
        "degree = 3\n",
        "polynomial_features = PolynomialFeatures(degree=degree, include_bias=True)\n",
        "linear_regression = LinearRegression()\n",
        "\n",
        "# Create a pipeline that includes both polynomial expansion and linear regression\n",
        "model = make_pipeline(polynomial_features, linear_regression)"
      ],
      "metadata": {
        "id": "w9kDNr96XtmW"
      },
      "id": "w9kDNr96XtmW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3, random_state=0)\n",
        "model.fit(X_train, y_train)\n",
        "train_score = model.score(X_train,y_train)\n",
        "test_score = model.score(X_test, y_test)\n",
        "print(f\"Average R2 Score on Training Data: {train_score}\")\n",
        "print(f\"Average R2 Score on Test Data: {test_score}\")"
      ],
      "metadata": {
        "id": "8aIHiLu_gPft"
      },
      "id": "8aIHiLu_gPft",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set number of splits for cross-validation - two years for each fold\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "n_splits = 2 # We have 40 years in total\n",
        "\n",
        "# Initialize KFold\n",
        "kf = KFold(n_splits=n_splits)\n",
        "\n",
        "# Arrays to store scores\n",
        "train_scores = []\n",
        "test_scores = []\n",
        "\n",
        "for train_index, test_index in kf.split(X):\n",
        "    # Split data\n",
        "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
        "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
        "    # Fit model\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Calculate R2 scores\n",
        "    train_score = model.score(X_train,y_train)\n",
        "    test_score = model.score(X_test, y_test)\n",
        "\n",
        "    # Append scores\n",
        "    train_scores.append(train_score)\n",
        "    test_scores.append(test_score)\n",
        "\n",
        "# Average R2 scores\n",
        "avg_train_score = np.mean(train_scores)\n",
        "avg_test_score = np.mean(test_scores)\n",
        "\n",
        "print(f\"Average R2 Score on Training Data: {avg_train_score}\")\n",
        "print(f\"Average R2 Score on Test Data: {avg_test_score}\")"
      ],
      "metadata": {
        "id": "EUglJMuGXvaM"
      },
      "id": "EUglJMuGXvaM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ridge Regression**"
      ],
      "metadata": {
        "id": "TZ0DRq1tPjWD"
      },
      "id": "TZ0DRq1tPjWD"
    },
    {
      "cell_type": "code",
      "source": [
        "#Ridge regression\n",
        "# Import Ridge\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.model_selection import cross_val_score\n",
        "# Call the Ridge regressor\n",
        "reg_class = Ridge\n",
        "\n",
        "# Number of test years\n",
        "N_TEST_YEARS = 8\n",
        "# Number of test days = number of test columns\n",
        "n_test = 365 * N_TEST_YEARS # 365 days per year\n",
        "\n",
        "# Define array of regularization-parameter values\n",
        "complexity_rng = np.linspace(0, 80, 80)\n",
        "\n",
        "# Select cross validation data\n",
        "X_cv = X[:-n_test]\n",
        "y_cv = y[:-n_test]\n",
        "\n",
        "# Select test set for later\n",
        "X_test = X[-n_test:]\n",
        "y_test = y[-n_test:]\n",
        "\n",
        "# Set number of splits for cross-validation - two years for each fold\n",
        "n_splits_cv = (n_years - N_TEST_YEARS)//2\n",
        "\n",
        "# Declare empty arrays in which to store r2 scores and coefficients\n",
        "r2_validation = np.empty(complexity_rng.shape)\n",
        "coefs = np.empty((len(complexity_rng), X.shape[1]))\n",
        "r2_test = np.empty(complexity_rng.shape)\n",
        "\n",
        "\n",
        "# Loop over regularization-parameter values\n",
        "for k, complexity in enumerate(complexity_rng):\n",
        "    # Define the Ridge estimator for particular regularization-parameter value\n",
        "    reg = reg_class(alpha=complexity)\n",
        "\n",
        "    # Get r2 test scores from k-fold cross-validation\n",
        "    r2_validation_arr = cross_val_score(reg, X_cv, y_cv, cv=n_splits_cv)\n",
        "\n",
        "    # Get r2 expected prediction score by averaging over test scores\n",
        "    r2_validation[k] = r2_validation_arr.mean()\n",
        "\n",
        "    # Save coefficients\n",
        "    reg.fit(X_cv, y_cv)\n",
        "    coefs[k] = reg.coef_\n",
        "\n",
        "    # Get r2 test error\n",
        "    r2_test[k] = reg.score(X_test, y_test)\n",
        "\n",
        "\n",
        "# Get the best values of the regularization parameter, prediction R2 and coefficients\n",
        "i_best = np.argmax(r2_validation)\n",
        "complexity_best = complexity_rng[i_best]\n",
        "r2_validation_best = r2_validation[i_best]\n",
        "coefs_best = coefs[i_best]\n",
        "r2_test_best = r2_test[i_best]"
      ],
      "metadata": {
        "id": "c0pmD7-JYeii"
      },
      "id": "c0pmD7-JYeii",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(r2_validation)"
      ],
      "metadata": {
        "id": "acleenFpSoLK"
      },
      "id": "acleenFpSoLK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot validation curve\n",
        "complexity_label = r'$\\alpha$'\n",
        "plt.figure()\n",
        "plt.plot(complexity_rng, r2_validation, label = 'Train R2')\n",
        "plt.legend()\n",
        "plt.xlabel(complexity_label)\n",
        "plt.ylabel(r'$R^2$')\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(complexity_rng, r2_test, label = 'Test R2')\n",
        "plt.xlabel(complexity_label)\n",
        "plt.ylabel(r'$R^2$')\n",
        "plt.legend()\n",
        "_ = plt.title(r'Best $R^2 train$: {:.3} for $\\alpha$ = {:.1e} and $R^2 test$ : {:.3}'.format(\n",
        "    r2_validation_best, complexity_best, r2_test_best))\n",
        "_ = plt.xlim(complexity_rng[[0, -1]])\n"
      ],
      "metadata": {
        "id": "DeN1ZSKLrT7_"
      },
      "id": "DeN1ZSKLrT7_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the Ridge estimator for best regularization parameter value\n",
        "reg = reg_class(alpha=complexity_best)\n",
        "\n",
        "# Fit on train data\n",
        "reg.fit(X_cv, y_cv)\n",
        "\n",
        "# Test on test data\n",
        "r2_test = reg.score(X_test, y_test)\n",
        "\n",
        "print('Test R2: {:.3f}'.format(r2_test))"
      ],
      "metadata": {
        "id": "m00xD4fuRw3l"
      },
      "id": "m00xD4fuRw3l",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "ad79471d",
      "metadata": {
        "id": "ad79471d"
      },
      "source": [
        "***\n",
        "## Credit\n",
        "\n",
        "[//]: # \"This notebook is part of [E4C Interdisciplinary Center - Education](https://gitlab.in2p3.fr/energy4climate/public/education).\"\n",
        "Contributors include Bruno Deremble and Alexis Tantet.\n",
        "Several slides and images are taken from the very good [Scikit-learn course](https://inria.github.io/scikit-learn-mooc/).\n",
        "\n",
        "<br>\n",
        "\n",
        "<div style=\"display: flex; height: 70px\">\n",
        "    \n",
        "<img alt=\"Logo LMD\" src=\"https://github.com/VTNay/MEC557-Project/blob/main/images/logos/logo_lmd.jpg?raw=1\" style=\"display: inline-block\"/>\n",
        "\n",
        "<img alt=\"Logo IPSL\" src=\"https://github.com/VTNay/MEC557-Project/blob/main/images/logos/logo_ipsl.png?raw=1\" style=\"display: inline-block\"/>\n",
        "\n",
        "<img alt=\"Logo E4C\" src=\"https://github.com/VTNay/MEC557-Project/blob/main/images/logos/logo_e4c_final.png?raw=1\" style=\"display: inline-block\"/>\n",
        "\n",
        "<img alt=\"Logo EP\" src=\"https://github.com/VTNay/MEC557-Project/blob/main/images/logos/logo_ep.png?raw=1\" style=\"display: inline-block\"/>\n",
        "\n",
        "<img alt=\"Logo SU\" src=\"https://github.com/VTNay/MEC557-Project/blob/main/images/logos/logo_su.png?raw=1\" style=\"display: inline-block\"/>\n",
        "\n",
        "<img alt=\"Logo ENS\" src=\"https://github.com/VTNay/MEC557-Project/blob/main/images/logos/logo_ens.jpg?raw=1\" style=\"display: inline-block\"/>\n",
        "\n",
        "<img alt=\"Logo CNRS\" src=\"https://github.com/VTNay/MEC557-Project/blob/main/images/logos/logo_cnrs.png?raw=1\" style=\"display: inline-block\"/>\n",
        "    \n",
        "</div>\n",
        "\n",
        "<hr>\n",
        "\n",
        "<div style=\"display: flex\">\n",
        "    <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-sa/4.0/\"><img alt=\"Creative Commons License\" style=\"border-width:0; margin-right: 10px\" src=\"https://i.creativecommons.org/l/by-sa/4.0/88x31.png\" /></a>\n",
        "    <br>This work is licensed under a &nbsp; <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-sa/4.0/\">Creative Commons Attribution-ShareAlike 4.0 International License</a>.\n",
        "</div>"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "latex_envs": {
      "LaTeX_envs_menu_present": true,
      "autoclose": true,
      "autocomplete": false,
      "bibliofile": "biblio.bib",
      "cite_by": "apalike",
      "current_citInitial": 1,
      "eqLabelWithNumbers": true,
      "eqNumInitial": 1,
      "hotkeys": {
        "equation": "Ctrl-E",
        "itemize": "Ctrl-I"
      },
      "labels_anchors": false,
      "latex_user_defs": false,
      "report_style_numbering": false,
      "user_envs_cfg": false
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}